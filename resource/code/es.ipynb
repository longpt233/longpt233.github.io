{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n",
    "# os.environ['PYSPARK_PYTHON'] = \"/data/dataanalyst/anaconda3/envs/data_env/bin/python\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = \"/data/dataanalyst/anaconda3/envs/data_env/bin/python\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = \"--jars='/home/ml_data_analyst/.ivy2/jars/' pyspark-shell\"\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages org.elasticsearch:elasticsearch-hadoop:7.1.0 pyspark-shell\"\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html\n",
    "# config này khi muốn chạy các thư viện python trên executor mà k cần tải trên các executor\n",
    "# (env_36) ml_data_analyst@hadoop532:~$ conda pack -f -o pyspark_conda_env_28_02_2023.tar.gz\n",
    "# chú ý cái #environment match với cái env PYSPARK_PYTHON ở trên\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--archives /home/ml_data_analyst/pyspark_conda_env_28_02_2023.tar.gz#environment pyspark-shell\"\n",
    "\n",
    "# nếu tải jar rồi add thì chỉ cần tải 1 lần, tuy nhiên sẽ hạn chế nếu muốn đổi version hay jh đó\n",
    "# pack dạng org:name:ver là chuẩn của ivy (có thể lấy trên maven)\n",
    "# --packages thi no se tai ve tai \n",
    "# (env_36) ml_data_analyst@hadoop532:~$ ll /home/ml_data_analyst/.ivy2/jars/\n",
    "# total 2980\n",
    "# drwxrwxr-x 2 ml_data_analyst ml_data_analyst    4096 Feb 28 10:18 ./\n",
    "# drwxrwxr-x 4 ml_data_analyst ml_data_analyst    4096 Feb 27 22:34 ../\n",
    "# -rw-rw-r-- 1 ml_data_analyst ml_data_analyst 1011702 May 16  2019 org.elasticsearch_elasticsearch-hadoop-7.1.0.jar\n",
    "# -rw-rw-r-- 1 ml_data_analyst ml_data_analyst 1011971 May 23  2019 org.elasticsearch_elasticsearch-hadoop-7.1.1.jar\n",
    "# -rw-rw-r-- 1 ml_data_analyst ml_data_analyst 1012404 Dec 17  2019 org.elasticsearch_elasticsearch-hadoop-7.5.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import findspark\n",
    "findspark.add_packages(\"org.elasticsearch.client:rest:5.1.2,org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0,commons-httpclient:commons-httpclient:3.1\")\n",
    "findspark.init('/opt/spark320')\n",
    "#org.apache.httpcomponents:httpclient:4.1.1,io.netty:netty-all:4.1.63.Final\n",
    "\n",
    "conf = SparkConf().setAppName('long test es')  \n",
    "conf.set('spark.yarn.queue', 'datamining')\n",
    "conf.set('spark.kerberos.principal', 'mlbigdata/dm@HADOOP.SECURE')\n",
    "conf.set('spark.kerberos.keytab', '/storage1/ml_data_analyst/mlbigdata.keytab')\n",
    "conf.set('spark.driver.memory', '5g')\n",
    "conf.set('spark.cores.max', '1')\n",
    "conf.set('spark.executor.memory', '1G')\n",
    "conf.set('spark.executor.instances', '5')\n",
    "conf.set('spark.kerberos.access.hadoopFileSystems', 'hdfs://172.18.5.86:9000')\n",
    "spark = SparkSession.builder.master(\"yarn\").config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myquery = \"\"\"{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"domain\": \"sport5.vn\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "es_reader = (spark.read\n",
    "    .format(\"org.elasticsearch.spark.sql\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"es.query\", myquery)  # filter trc load se nhanh hon\n",
    "    .option(\"pushdown\", \"true\")\n",
    "    .option(\"es.read.field.include\", \"content,url,domain\")\n",
    "    .option(\"es.read.field.exclude\", \"\")  # nếu có trường trong list bên trên thì bỏ qua\n",
    "    .option(\"es.nodes\",\"10.3.49.177:9200\")\n",
    "    .option(\"es.field.read.empty.as.null\",\"no\")\n",
    "    .option(\"es.read.metadata\",True)\n",
    "    .option(\"es.net.http.auth.user\",\"urldata\")\n",
    "    .option(\"es.net.http.auth.pass\",\"3K75Fu1zijmanWP7OSl\")\n",
    ")\n",
    "\n",
    "# .option('es.read.field.as.array.include','*')   # Caused by: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of bigint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sysmon_df = es_reader.load(\"urldata_2022_04\")\n",
    "sysmon_df.select(\"_metadata\").show(2, False)\n",
    "id_doc = sysmon_df.select(sysmon_df._metadata._id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def get_es():\n",
    "    from elasticsearch import Elasticsearch\n",
    "    host = \"10.1.11.1\"\n",
    "    port = \"9200\"\n",
    "    username = \"user\"\n",
    "    password = \"pass\"\n",
    "\n",
    "    es = Elasticsearch(\n",
    "            [f\"{host}:{port}\"],\n",
    "            http_auth=(username, password) \n",
    "        )\n",
    "\n",
    "        # Kiểm tra xem kết nối có thành công hay không\n",
    "    if es.ping():\n",
    "        print(\"Kết nối đến Elasticsearch cluster thành công.\")\n",
    "        return es\n",
    "    else:\n",
    "        print(\"Không thể kết nối đến Elasticsearch cluster.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(iter_row):\n",
    "    es = get_es()\n",
    "    print(es.info())\n",
    "    out = []\n",
    "    for row in iter_row: \n",
    "        doc = es.get(index=\"urldata_2022_04\", id=row['_metadata']['_id'])\n",
    "        out.append((doc['_source']['timeCreate'],1))\n",
    "        \n",
    "    return outb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = sysmon_df.rdd.mapPartitions(process_partition).cache()\n",
    "print(result_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "deptSchema = StructType([       \n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(result_rdd, schema = deptSchema)\n",
    "deptDF1.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
