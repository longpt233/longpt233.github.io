{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# them file jar\n",
    "# findspark.add_packages(\"org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0,org.apache.httpcomponents:httpclient:4.1.1\")\n",
    "findspark.init('/opt/spark320') # để ăn config spark \n",
    "\n",
    "conf = SparkConf().setAppName('check hadoop')\n",
    "conf.set('spark.yarn.queue', 'plat')\n",
    "conf.set('spark.kerberos.principal', 'long/plat@HADOOP.SECURE')\n",
    "conf.set('spark.kerberos.keytab', 'path')\n",
    "conf.set('spark.kerberos.access.hadoopFileSystems', 'hdfs://172.1.1.1:9000')\n",
    "conf.set('spark.sql.parquet.binaryAsString', 'true') # nếu không set thì có thể có lúc đọc file ra binary type\n",
    "\n",
    "conf.set('spark.port', '4071')\n",
    "\n",
    "conf.set('spark.driver.memory', '2g') \n",
    "conf.set('spark.executor.memory', '2G')\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[8]\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# conf.set('spark.cores.max', '1')\n",
    "# conf.set('spark.executor.instances', '5')\n",
    "# spark = SparkSession.builder.master(\"yarn\").config(conf=conf).getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# $ conda activate env_36 \n",
    "# $ source /opt/spark320/conf/spark-env.sh # chú ý phải chạy lệnh trên khi tạo mới môi trường, nếu k spark trên yarn sẽ k ăn config PYSPARK_PYTHON và PYSPARK_DRIVER_PYTHON của spark\n",
    "# $ jupyter-notebook --ip=0.0.0.0 --port=9090\n",
    "\n",
    "print(os.environ['PYSPARK_PYTHON'])  # giống trong file spark-env. vì trên các executor mới có cái path này\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n",
    "\n",
    "# để config như trên thì chạy được pyspark trên yarn, tuy nhiên k  chạy được local[] vì nó đang ăn env của spark-env, k ăn path của env chạy notebook hiện tại. lệnh set lại để chạy local[]\n",
    "os.environ['PYSPARK_PYTHON'] = \"/storage/long/anaconda3/envs/env_36/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/storage/long/anaconda3/envs/env_36/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uv = spark.read.parquet(\"hdfs://172.1.1.1.:9000/data/{2023_{10,11,12}_*,2024_{01,02}_*}/*\")\n",
    "df_uv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition\n",
    "\n",
    "from aerospike import client # chịu k import dc\n",
    "def process_partition(iter_row):\n",
    "\n",
    "    config = {\n",
    "        'hosts': [('1.1.1.1',3000)] \n",
    "    }\n",
    "    client_instance = client(config).connect()\n",
    "    namespace = 'mem_hadoop'\n",
    "    dataset = 'user_profile'\n",
    "\n",
    "    def get(key):\n",
    "        key = (namespace, dataset, key)\n",
    "        try:\n",
    "            (key, metadata, record) = client_instance.get(key)\n",
    "            return 1\n",
    "        except Exception as e: \n",
    "    #         print(\"read error \", e)\n",
    "            return -1\n",
    "\n",
    "    out = []\n",
    "    # Iterate over the records in the partition\n",
    "    for row in iter_row: \n",
    "\n",
    "        user_id = row[0]\n",
    "        exits = get(user_id)\n",
    "        row_out = (user_id,exits)\n",
    "        out.append(row_out)\n",
    "        \n",
    "\n",
    "    client_instance.close()\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# df_uv schema : user_id, uv ...\n",
    "result_rdd = df_uv.rdd.mapPartitions(process_partition)\n",
    "result_rdd.cache().count()\n",
    "res_df = result_rdd.toDF((\"userId\", \"exits\"))\n",
    "res.filter(\"exits ==1\").count()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
